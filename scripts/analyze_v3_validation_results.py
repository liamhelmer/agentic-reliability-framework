#!/usr/bin/env python3
"""
Analyze V3 Validation Results from GitHub Actions

This script helps analyze the results of V3 boundary validation runs
by examining workflow artifacts and providing actionable insights.
"""

import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import sys


class V3ValidationAnalyzer:
    """Analyze V3 validation results from GitHub Actions."""
    
    def __init__(self):
        self.results_dir = Path(__file__).parent.parent / ".github" / "workflow-results"
        self.results_dir.mkdir(exist_ok=True)
        
        # Expected validation outcomes
        self.expected_checks = [
            {
                "name": "OSS Boundary Check",
                "script": "oss_boundary_check.py",
                "critical": True,
                "description": "Validates OSS repository purity"
            },
            {
                "name": "OSS Purity Enforcement",
                "script": "enforce_oss_purity.py",
                "critical": True,
                "description": "Enforces OSS code boundaries"
            },
            {
                "name": "Enhanced V3 Boundary Check",
                "script": "enhanced_v3_boundary_check.py",
                "critical": True,
                "description": "Validates V3 architectural boundaries"
            },
            {
                "name": "V3 Boundary Integration",
                "script": "v3_boundary_integration.py",
                "critical": True,
                "description": "Integrates all V3 validations"
            },
            {
                "name": "Comprehensive V3 Validation",
                "script": "run_v3_validation.py",
                "critical": True,
                "description": "Runs all V3 validations with certification"
            },
        ]
    
    def analyze_workflow_run(self, run_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Analyze a workflow run (simulated since we can't access API).
        
        In production, this would use GitHub API to fetch actual logs.
        For now, we analyze local test results.
        """
        print("üîç Analyzing V3 Validation Results")
        print("=" * 60)
        
        # Check if certification file exists
        cert_path = Path(__file__).parent.parent / "V3_COMPLIANCE_CERTIFICATION.json"
        issues_path = Path(__file__).parent.parent / "V3_COMPLIANCE_ISSUES.json"
        
        analysis = {
            "timestamp": datetime.utcnow().isoformat(),
            "certification_found": cert_path.exists(),
            "issues_found": issues_path.exists(),
            "checks": [],
            "overall_status": "UNKNOWN",
            "recommendations": [],
        }
        
        # Analyze certification if it exists
        if cert_path.exists():
            try:
                with open(cert_path, 'r') as f:
                    certification = json.load(f)
                
                analysis["certification"] = certification
                analysis["overall_status"] = "CERTIFIED" if certification.get("compliance_levels", {}).get("v3_0_advisory_intelligence") == "VERIFIED" else "ISSUES"
                
                print(f"‚úÖ Found V3 Certification: {cert_path.name}")
                print(f"   Version: {certification.get('version', 'Unknown')}")
                print(f"   Timestamp: {certification.get('timestamp', 'Unknown')}")
                
                # Check compliance levels
                print("\nüìä Compliance Levels:")
                for level, status in certification.get("compliance_levels", {}).items():
                    icon = "‚úÖ" if status == "VERIFIED" else "‚è≥"
                    readable = level.replace("_", " ").title()
                    print(f"   {icon} {readable}: {status}")
                
                # Check boundary verification
                print("\nüîí Boundary Verification:")
                for boundary, status in certification.get("boundary_verification", {}).items():
                    icon = "‚úÖ" if status == "VERIFIED" else "‚è≥"
                    readable = boundary.replace("_", " ").title()
                    print(f"   {icon} {readable}: {status}")
                
            except Exception as e:
                analysis["certification_error"] = str(e)
                print(f"‚ö†Ô∏è  Error reading certification: {e}")
        
        elif issues_path.exists():
            try:
                with open(issues_path, 'r') as f:
                    issues = json.load(f)
                
                analysis["issues"] = issues
                analysis["overall_status"] = "FAILED"
                
                print(f"‚ö†Ô∏è  Found V3 Compliance Issues: {issues_path.name}")
                print("\nüîß Issues Detected:")
                
                # Extract failed checks
                if issues.get("existing_check", {}).get("passed") is False:
                    print("   ‚Ä¢ Existing OSS boundary check failed")
                    analysis["recommendations"].append("Fix existing OSS boundary violations")
                
                if issues.get("v3_check", {}).get("passed") is False:
                    print("   ‚Ä¢ Enhanced V3 boundary check failed")
                    analysis["recommendations"].append("Fix V3 architectural boundary violations")
                
            except Exception as e:
                analysis["issues_error"] = str(e)
                print(f"‚ö†Ô∏è  Error reading issues file: {e}")
        else:
            print("üìù No V3 compliance files found")
            print("   Run: python scripts/run_v3_validation.py --certify")
            analysis["recommendations"].append("Run comprehensive V3 validation")
        
        # Check for test result files
        test_results = self._check_test_results()
        analysis["test_results"] = test_results
        
        if test_results.get("all_passed"):
            print(f"\n‚úÖ All local tests passed: {test_results.get('passed_count')}/{test_results.get('total_count')}")
        else:
            print(f"\n‚ùå Test failures: {test_results.get('failed_count', 0)} failed")
            analysis["recommendations"].append("Fix failing tests before proceeding")
        
        # Generate recommendations
        if analysis["overall_status"] == "CERTIFIED":
            print("\nüéâ V3.0 ADVISORY INTELLIGENCE LOCK-IN VERIFIED")
            print("\n‚úÖ Ready for V3.0 release with:")
            print("   ‚Ä¢ Mechanical OSS/Enterprise boundaries")
            print("   ‚Ä¢ Advisory-only execution in OSS")
            print("   ‚Ä¢ Proper license enforcement")
            
            analysis["recommendations"].extend([
                "Proceed with V3.0 OSS package release",
                "Update documentation with V3 boundaries",
                "Announce V3 architecture to community",
            ])
        elif analysis["overall_status"] == "FAILED":
            print("\nüö® V3 VALIDATION FAILED")
            print("\nRequired actions:")
            for rec in analysis["recommendations"]:
                print(f"   ‚Ä¢ {rec}")
            
            analysis["recommendations"].extend([
                "Run individual validation scripts for details",
                "Check GitHub Actions logs for specific errors",
                "Fix boundary violations before retrying",
            ])
        
        return analysis
    
    def _check_test_results(self) -> Dict[str, Any]:
        """Check local test results."""
        test_dir = Path(__file__).parent.parent / "tests"
        results = {
            "total_count": 0,
            "passed_count": 0,
            "failed_count": 0,
            "all_passed": True,
            "details": [],
        }
        
        if not test_dir.exists():
            return results
        
        # Look for test files
        test_files = list(test_dir.glob("test_*.py"))
        results["total_count"] = len(test_files)
        
        # Check for specific test files
        critical_tests = [
            "test_oss_purity.py",
            "test_imports.py",
            "test_basic.py",
        ]
        
        for test_file in critical_tests:
            test_path = test_dir / test_file
            if test_path.exists():
                # Simulate test check (in production would run pytest)
                results["details"].append({
                    "test": test_file,
                    "exists": True,
                    "status": "UNKNOWN"  # Would be "PASSED" or "FAILED" from actual run
                })
            else:
                results["details"].append({
                    "test": test_file,
                    "exists": False,
                    "status": "MISSING"
                })
                results["all_passed"] = False
        
        return results
    
    def generate_validation_report(self) -> Path:
        """Generate a comprehensive validation report."""
        analysis = self.analyze_workflow_run()
        
        report_path = self.results_dir / f"v3_validation_report_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(report_path, 'w') as f:
            json.dump(analysis, f, indent=2)
        
        print(f"\nüìÑ Report saved to: {report_path.relative_to(Path.cwd())}")
        
        # Generate markdown summary
        md_path = report_path.with_suffix('.md')
        self._generate_markdown_report(analysis, md_path)
        
        return report_path
    
    def _generate_markdown_report(self, analysis: Dict[str, Any], output_path: Path):
        """Generate markdown report from analysis."""
        with open(output_path, 'w') as f:
            f.write("# V3 Validation Report\n\n")
            f.write(f"**Generated**: {analysis['timestamp']}\n")
            f.write(f"**Status**: {analysis['overall_status']}\n\n")
            
            if analysis.get("certification_found"):
                f.write("## ‚úÖ V3 Certification Found\n\n")
                cert = analysis.get("certification", {})
                f.write(f"- **Version**: {cert.get('version', 'Unknown')}\n")
                f.write(f"- **Timestamp**: {cert.get('timestamp', 'Unknown')}\n\n")
                
                f.write("### Compliance Levels\n")
                for level, status in cert.get("compliance_levels", {}).items():
                    f.write(f"- **{level.replace('_', ' ').title()}**: {status}\n")
                
                f.write("\n### Boundary Verification\n")
                for boundary, status in cert.get("boundary_verification", {}).items():
                    f.write(f"- **{boundary.replace('_', ' ').title()}**: {status}\n")
            
            elif analysis.get("issues_found"):
                f.write("## ‚ö†Ô∏è V3 Compliance Issues Found\n\n")
                f.write("### Recommendations\n")
                for rec in analysis.get("recommendations", []):
                    f.write(f"- {rec}\n")
            
            if analysis.get("recommendations"):
                f.write("\n## üöÄ Next Steps\n\n")
                for i, rec in enumerate(analysis["recommendations"], 1):
                    f.write(f"{i}. {rec}\n")
            
            f.write("\n---\n")
            f.write("*Report generated by V3 Validation Analyzer*")
        
        print(f"üìù Markdown report: {output_path.relative_to(Path.cwd())}")


def main():
    """Main analysis entry point."""
    analyzer = V3ValidationAnalyzer()
    
    print("=" * 70)
    print("üìä V3 VALIDATION RESULTS ANALYZER")
    print("=" * 70)
    print("\nThis script analyzes V3 boundary validation results and provides")
    print("actionable insights for the V3.0 release.\n")
    
    # Analyze current state
    analysis = analyzer.analyze_workflow_run()
    
    # Generate comprehensive report
    report_path = analyzer.generate_validation_report()
    
    # Provide next steps based on analysis
    print("\n" + "=" * 70)
    print("üéØ RECOMMENDED NEXT STEPS")
    print("=" * 70)
    
    if analysis["overall_status"] == "CERTIFIED":
        print("\n‚úÖ V3.0 VALIDATION COMPLETE")
        print("\nYou can now proceed with:")
        print("1. V3.0 OSS Package Release")
        print("   - Update version to 3.0.0 in __version__.py")
        print("   - Update changelog with V3 boundaries")
        print("   - Release to PyPI")
        
        print("\n2. Documentation Updates")
        print("   - Update README with V3 architecture")
        print("   - Document OSS vs Enterprise boundaries")
        print("   - Create migration guide from v2")
        
        print("\n3. Community Announcement")
        print("   - Prepare V3 announcement")
        print("   - Highlight advisory-only OSS features")
        print("   - Showcase Enterprise execution governance")
        
    elif analysis["overall_status"] == "FAILED":
        print("\nüö® V3 VALIDATION INCOMPLETE")
        print("\nRequired actions:")
        for i, rec in enumerate(analysis["recommendations"][:5], 1):
            print(f"{i}. {rec}")
        
        print("\nDebug steps:")
        print("1. Check GitHub Actions logs for specific errors")
        print("2. Run validation scripts individually:")
        print("   python scripts/enhanced_v3_boundary_check.py")
        print("   python scripts/v3_boundary_integration.py")
        print("3. Examine generated compliance files")
    
    else:
        print("\nüìã RUN COMPREHENSIVE VALIDATION")
        print("\nRun the full validation suite:")
        print("python scripts/run_v3_validation.py --certify")
        print("\nThis will:")
        print("- Run all boundary checks")
        print("- Generate V3 compliance certification")
        print("- Identify any remaining issues")
    
    print(f"\nüìÑ Full analysis report: {report_path.relative_to(Path.cwd())}")
    
    # Return appropriate exit code
    if analysis["overall_status"] == "CERTIFIED":
        sys.exit(0)
    else:
        sys.exit(1)


if __name__ == "__main__":
    main()
